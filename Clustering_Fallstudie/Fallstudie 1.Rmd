---
title: "Fallstudie 1"
author: "Sanja Perovic, Julia Zerrweck"
date: "7/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```
#kk
# Data preparation
## a
```{r}
library(readr)
df <- read_csv("responses.csv")
df
```

```{r}
library(tidyr)
df <- drop_na(df)
df
```

```{r}
library(tidyverse)
glimpse(df)
```


## b
```{r}
library(dplyr)
df_1 <- select (df, c("Height",
                      "Weight",
                      "Gender"))
df_1
```

## c
```{r}
library(ggplot2)
x <- df_1$Height
y <- df_1$Weight

  plot (
    x, y, main = "scatterplot",
    xlab = "Height",
    ylab = "Weight")
```

## d
```{r}
x <- select (df_1, c("Height",
                     "Weight"))

x$Height_sc <- scale (x$Height,
                      scale =TRUE)

x$Weight_sc <- scale (x$Weight,
                      scale =TRUE)

x_std <- select(x, c("Height_sc",
                     "Weight_sc"))
x_std
```


```{r}
x <- x_std$Height_sc
y <- x_std$Weight_sc

  plot (
    x, y, main = "scatterplot_sc",
    xlab = "Height",
    ylab = "Weight")
```

## e
Um Werte, die sich in unterschiedlichen Wertebereichen befinden, vergleichen zu können muss der Mittelwert auf 0 und die Standardabweichung auf 1 gesetzt werden.
Die Abstände zwischen den Werten machen es sonst unmöglich diese direkt vergleichen zu können. Wir können in unserem Fall erkennen, wie sich die Werte auf den x&y-Achsen deutlich vermindert haben.

# k-Means

## a
```{r}
set.seed(123)
x_cluster <- kmeans(x_std, 2, nstart = 8)
x_cluster
```

## b

Unser Verfahren startet von 8 verschiedenen Starting Points. Denn je höher der Starting Point desto eher können wir ein stabileres Resultat erhalten.
kMeans läuft nämlich 8 Mal in unserem Fall durch und kann aus diesen 8 Resultaten das beste Ergebnis abliefern.

## c

total SS = Summe der  Quadrate
between SS = Summe der quadrierten Abstände der Quadrate

60.3% = die totale Varianz im Dataset

## d

```{r}
x_std$gender_kmeans <- as.factor(x_cluster$cluster)
x_std
```

## e

```{r}
ggplot(x_std, 
       aes(Height_sc, Weight_sc, color = gender_kmeans)) + 
  geom_point()

```

Das Scatterplot wird in die zwei Cluster unterteilt, die hier farbig zum Vorschein kommen. 
Der Verlauf (Richtung) des Scatterplots ist positiv. 
Art: linear, da die Werte auf beiden Achsen zunehmen.
Stärke: geringe Streuung

# Hierarchical Clustering (Ward)

## a
```{r}
d <- 
  x_std %>% 
  select(-gender_kmeans) %>% 
  dist(method = "euclidean")
d
```


```{r}
hc <- hclust(d, method = "ward.D2") 
```

```{r}
sort(unique(cophenetic(hc)))
```

## b
```{r}
plot (hc)
```

```{r}
plot(hc, hang = -2, cex = 0.6)
hc$labels <- x_std$Weight_sc
rect.hclust(hc, k = 2, border = "red")
```

```{r}
gruppen <- cutree(hc, k = 2) 
```


## d
```{r}
x_std$gender_ward <- gruppen
```

## e
```{r}
ggplot(x_std, 
       aes(Height_sc, Weight_sc, color = gender_ward)) + 
  geom_point()
```

Das Scatterplot wird in die zwei Cluster unterteilt, die hier farbig zum Vorschein kommen. 
Der Verlauf (Richtung) des Scatterplots ist positiv. 
Art: linear, da die Werte auf beiden Achsen zunehmen.
Stärke: geringe Streuung der Werte.

# Compare results

## a
### 1
```{r}
ggplot(df_1, 
       aes(Height, Weight, color = Gender)) + 
  geom_point()
```

### 2
```{r}
ggplot(df_1, 
       aes(Height, Weight, color = x_std$gender_kmeans)) + 
  geom_point()
```

### 3
```{r}
ggplot(df_1, 
       aes(Height, Weight, color = x_std$gender_ward)) + 
  geom_point()
```

K-Means-Clustering ist ein schneller, robuster und einfacher Algorithmus, der zuverlässige Ergebnisse liefert, wenn die Datensätze sich voneinander unterscheiden.Er wird am besten verwendet, wenn die Anzahl der Clusterzentren aufgrund einer gut definierten Liste von Typen, die in den Daten angezeigt werden, spezifiziert ist. Es ist jedoch zu bedenken, dass das K-Means-Clustering möglicherweise nicht gut funktioniert, wenn es z.B. stark überlappende Daten enthält oder diese voller Ausreißer sind.
k-means eine hervorragende Lösung für das Pre-Clustering, da es den Platz in  kleinere Unterräume reduziert, in denen andere Clustering-Algorithmen angewendet werden können. 
Ward Methode nutzt keine random Starting Points, das Ergebnis ist also eindeutig und das Verfahren wird meistens dann veerwendet, wenn es keine Ausreißer gibt und die Cluster ungefährt gleich groß sein sollen.

Die Ward Methode hat in unserem Fall mehr Männer erreichen können und hat einen deutlich härteren Cut gemacht als kmeans. kmeans hat die zwei Cluster so unterteilt, dass die größeren Ausreißer der Frauen einfach dem 2. Cluster zugeordnet wurden.

Aufgrund der Menge der Daten und der leichten Cluster Bestimmung tendieren wir zu kmeans, da man bei der kmeans Analyse mehr Frauen Daten visualisert hat.


# Finalize data
```{r}
df_3 <- x_std %>%
  filter(gender_kmeans == "1")
```

